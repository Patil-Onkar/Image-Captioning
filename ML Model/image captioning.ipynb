{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of exp3 image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFI_AHc1m9d4"
      },
      "source": [
        "# Extracting features and ready to train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yug52xboykBz"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import collections\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUh_3jPxyxxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada996b4-d590-414a-b95d-b9d97c2dd310"
      },
      "source": [
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 6s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 326s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wXbaFQQy11x"
      },
      "source": [
        "with open('/content/annotations/captions_train2014.json', 'r') as f:\n",
        "    annotations = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbAZGfrg3fjT"
      },
      "source": [
        "# Group all captions together having the same image ID.\n",
        "PATH='/content/train2014/'\n",
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for val in annotations['annotations']:\n",
        "  caption = f\"<start> {val['caption']} <end>\"\n",
        "  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
        "  image_path_to_caption[image_path].append(caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfGiOHCP3jAf"
      },
      "source": [
        "\n",
        "image_paths = list(image_path_to_caption.keys())\n",
        "random.shuffle(image_paths)\n",
        "image_paths=image_paths[:30000]\n",
        "\n",
        "# Don't run this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dspTiXXAMnjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5hM1T2jeMpl"
      },
      "source": [
        "# storing the image path\n",
        "\n",
        "\n",
        "with open('image_paths', 'wb') as fp:\n",
        "    pickle.dump(image_paths, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptI4ZKVFe0rA"
      },
      "source": [
        "with open ('image_paths', 'rb') as fp:\n",
        "    img_pths = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xurF7kHSfDSG",
        "outputId": "8aeb125c-1179-4bcb-f25c-ce516cff8c59"
      },
      "source": [
        "for i in range(5):\n",
        "  a=random.randint(0,20000)\n",
        "  assert img_pths[a]==image_paths[a], 'something went wrong'\n",
        "  print('correct!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct!!\n",
            "correct!!\n",
            "correct!!\n",
            "correct!!\n",
            "correct!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRnmRRp_fDPf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QCwhY6KnIAf"
      },
      "source": [
        "# Inception model for feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA2jtVYHnO5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48abc40c-5853-46ed-ea5e-0f784d85880f"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "\n",
        "inception_feature_extraction=tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k_x4Cf0nW6M"
      },
      "source": [
        "# Saving the feature in trainable format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqjUcUk8nO2w"
      },
      "source": [
        "def fe(img):\n",
        "  i=tf.io.read_file(img)\n",
        "  i=tf.image.decode_jpeg(i, channels=3)\n",
        "  i=tf.image.resize(i, (299, 299))\n",
        "  i=tf.keras.applications.inception_v3.preprocess_input(i)\n",
        "  i=tf.expand_dims(i,0)\n",
        "  i=inception_feature_extraction(i)\n",
        "  i=tf.reshape(i,(64,2048))\n",
        "  return i.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO5XmlAynO0Y",
        "outputId": "f949c85d-eef7-4692-ee40-b205cadd9454"
      },
      "source": [
        "# Feature extraction from pretrained Inception model\n",
        "\n",
        "#train_features=[]\n",
        "k=0\n",
        "for i in image_paths:\n",
        "  k=k+1\n",
        "  np.save(i,fe(i))\n",
        "  #train_features.append(fe(i))\n",
        "  if k%500 ==0:\n",
        "    print(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n",
            "7500\n",
            "8000\n",
            "8500\n",
            "9000\n",
            "9500\n",
            "10000\n",
            "10500\n",
            "11000\n",
            "11500\n",
            "12000\n",
            "12500\n",
            "13000\n",
            "13500\n",
            "14000\n",
            "14500\n",
            "15000\n",
            "15500\n",
            "16000\n",
            "16500\n",
            "17000\n",
            "17500\n",
            "18000\n",
            "18500\n",
            "19000\n",
            "19500\n",
            "20000\n",
            "20500\n",
            "21000\n",
            "21500\n",
            "22000\n",
            "22500\n",
            "23000\n",
            "23500\n",
            "24000\n",
            "24500\n",
            "25000\n",
            "25500\n",
            "26000\n",
            "26500\n",
            "27000\n",
            "27500\n",
            "28000\n",
            "28500\n",
            "29000\n",
            "29500\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQi6_1zAAla3"
      },
      "source": [
        "train_captions = []\n",
        "img_pths=[]\n",
        "for i in range(len(image_paths)):\n",
        "  caption_list = image_path_to_caption[image_paths[i]]\n",
        "  train_captions.extend(caption_list)\n",
        "  img_pths.extend([image_paths[i]] * len(caption_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsR63Eh0fUN3",
        "outputId": "664d8e9e-e2b0-48cb-f6d9-3d6f9af8ba0c"
      },
      "source": [
        "len(img_pths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEDqQlTjJRor",
        "outputId": "a9533503-ea31-4267-f079-2035909d3d8c"
      },
      "source": [
        "# Checking if inception extraction is convertible to tflite\n",
        "\n",
        "path='/content/drive/MyDrive/models/Image_captioning app/2'\n",
        "tf.saved_model.save(inception_feature_extraction,path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/Image_captioning app/2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwP6vSadOYSl"
      },
      "source": [
        "converter=tf.lite.TFLiteConverter.from_saved_model(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzjl0WjCOtKU",
        "outputId": "9f5e9a26-a1b1-46e2-e3e3-30d2854d3e15"
      },
      "source": [
        "imported = tf.saved_model.load(path)\n",
        "print(imported.signatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, input_1) at 0x7F2788C17550>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlSxhwFNOtFE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhgP1r_VCo2H"
      },
      "source": [
        "# Caption Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSuVYgiEAlXz"
      },
      "source": [
        "# Tokenizing text\n",
        "tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=5000, \n",
        "                                                    oov_token='<oov>',\n",
        "                                                    filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AljmafA8AlUs",
        "outputId": "a61bdda7-1ead-4176-e8eb-b188d839c4d6"
      },
      "source": [
        "# Lets see <oov> distribution \n",
        "cap=tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "print('Checking the oov distribution')\n",
        "\n",
        "h=[i.count(1) for i in cap]\n",
        "plt.hist(h ,bins=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the oov distribution\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOFjLlq8Fp88"
      },
      "source": [
        "We can see that very few sentences has oov token more than 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "wY7vVsWUAlRF",
        "outputId": "7ad05c59-9bf1-45d4-f4ef-60939da0199a"
      },
      "source": [
        "# Length distribution\n",
        "l=[len(i) for i in cap]\n",
        "plt.hist(l ,bins=30)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSytStVXfM04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21567e4f-3509-48a0-e25f-d59d987636ae"
      },
      "source": [
        "# lets pad this upto len 22\n",
        "\n",
        "cpt=tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    cap, maxlen=23, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "# add pad token to dict\n",
        "tokenizer.word_index['<pad>']=0\n",
        "print('shape of final processed captions :',cpt.shape)\n",
        "wd_itow={i:j for j,i in tokenizer.word_index.items()}\n",
        "\n",
        "print('<start> and <end> token no are {} and {} resp'.format(tokenizer.word_index['<start>'],tokenizer.word_index['<end>']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of final processed captions : (150059, 23)\n",
            "<start> and <end> token no are 3 and 4 resp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHenDKpXEZlw"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/models/Image_captioning app/dict.json','w') as f:\n",
        "  json.dump(d,f,indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV4CWF2DgRVm",
        "outputId": "d8159332-7618-4246-b194-f6d811938067"
      },
      "source": [
        "cpt.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150059, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Doxwq2vCdMJ"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po46ZhbhCtyO"
      },
      "source": [
        "def mapfn(x,y):\n",
        "  img_tns=np.load(x.decode('utf-8')+'.npy')\n",
        "  return img_tns,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdzjbqM7CtvP"
      },
      "source": [
        "\n",
        "trn=tf.data.Dataset.from_tensor_slices((img_pths,cpt))\n",
        "trn = trn.map(lambda item1, item2: tf.numpy_function(\n",
        "          mapfn, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG15l9JQkdQ0"
      },
      "source": [
        "trn=trn.shuffle(1000,reshuffle_each_iteration=True).batch(64,drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU-_CQFqLN_3"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY2L-S5zLNtw"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(CNN_Encoder,self).__init__()\n",
        "    #self.fc1=tf.keras.layers.Dense(512)\n",
        "    self.fc = tf.keras.layers.Dense(256,activation='relu')\n",
        "\n",
        "  def call(self,inputs):\n",
        "    #out=self.fc1(inputs)\n",
        "    out=self.fc(inputs)\n",
        "    return out\n",
        "\n",
        "enc=CNN_Encoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3O7tFKhYcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386ddfad-761e-4e82-9edf-272361b8be74"
      },
      "source": [
        "# Checking the correctness of encoder\n",
        "inp=trn_ftr[0].reshape((1,64,2048))\n",
        "out=enc(inp)\n",
        "\n",
        "assert out.shape==(1,64,256),'Something went wrong'\n",
        "print('looks fine!!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks fine!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdeEx-a4mVSX",
        "outputId": "b3f1dad3-fa9a-4321-94f5-b246e6a4d9e8"
      },
      "source": [
        "enc.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"cnn__encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  524544    \n",
            "=================================================================\n",
            "Total params: 524,544\n",
            "Trainable params: 524,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f758IIyuPMut",
        "outputId": "70430c77-f4fd-4f96-d70d-9fa3e49924c0"
      },
      "source": [
        "tf.saved_model.save(enc,path)\n",
        "converter=tf.lite.TFLiteConverter.from_saved_model(path)\n",
        "imported = tf.saved_model.load(path)\n",
        "print(imported.signatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/Image_captioning app/2/assets\n",
            "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, input_1) at 0x7F2782116710>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KukhShLPMsB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMSaASZPPMou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ArIH6u3PMj-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdWkH0NhYaQ"
      },
      "source": [
        "# Word Alignment\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self,units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.w1=tf.keras.layers.Dense(units)\n",
        "    self.w2=tf.keras.layers.Dense(units)\n",
        "    self.v=tf.keras.layers.Dense(1)\n",
        "    self.activation = tf.keras.layers.Activation('tanh')\n",
        "\n",
        "  def call (self,inputs):\n",
        "    dec_hid,enc_out=inputs\n",
        "    x=self.w1(dec_hid)+self.w2(enc_out)\n",
        "    x=self.activation(x)\n",
        "    x=self.v(x)\n",
        "    wghts=tf.keras.activations.softmax(x,axis=1)\n",
        "    context_vector=wghts*enc_out\n",
        "    context_vector=tf.reduce_sum(context_vector,axis=1)\n",
        "    return wghts,context_vector\n",
        "\n",
        "at=BahdanauAttention(256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2603xS9hYXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f558f71-6690-4c99-9aec-19c88f1d7741"
      },
      "source": [
        "#Testing the attention layer\n",
        "\n",
        "dec_hid = tf.zeros((1,1,256))\n",
        "\n",
        "inputs=[dec_hid,out]\n",
        "weights,context_vector=at(inputs)\n",
        "assert weights.shape==(1,64,1), 'Somthing went wrong with weights'\n",
        "assert context_vector.shape==(1,256), 'Somthing went wrong with context_vector'\n",
        "print('looks fine')\n",
        "\n",
        "# Printing the weights\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks fine\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 64, 1), dtype=float32, numpy=\n",
              "array([[[0.01052245],\n",
              "        [0.01249215],\n",
              "        [0.01772313],\n",
              "        [0.02253019],\n",
              "        [0.0234569 ],\n",
              "        [0.0197035 ],\n",
              "        [0.01545711],\n",
              "        [0.0084044 ],\n",
              "        [0.0128596 ],\n",
              "        [0.01129285],\n",
              "        [0.01266724],\n",
              "        [0.01451955],\n",
              "        [0.01194725],\n",
              "        [0.0129849 ],\n",
              "        [0.00787702],\n",
              "        [0.00682098],\n",
              "        [0.01326701],\n",
              "        [0.00897275],\n",
              "        [0.00641177],\n",
              "        [0.01184812],\n",
              "        [0.01088392],\n",
              "        [0.01253196],\n",
              "        [0.00858605],\n",
              "        [0.01785402],\n",
              "        [0.02022077],\n",
              "        [0.02018351],\n",
              "        [0.01994715],\n",
              "        [0.02016301],\n",
              "        [0.01278407],\n",
              "        [0.00916066],\n",
              "        [0.00751155],\n",
              "        [0.01776459],\n",
              "        [0.01824783],\n",
              "        [0.01809319],\n",
              "        [0.03209174],\n",
              "        [0.03004121],\n",
              "        [0.0203446 ],\n",
              "        [0.00704611],\n",
              "        [0.00666097],\n",
              "        [0.02177604],\n",
              "        [0.01590361],\n",
              "        [0.01630157],\n",
              "        [0.02227497],\n",
              "        [0.01472211],\n",
              "        [0.01671814],\n",
              "        [0.01434031],\n",
              "        [0.01077259],\n",
              "        [0.0145636 ],\n",
              "        [0.02104144],\n",
              "        [0.01890405],\n",
              "        [0.01428037],\n",
              "        [0.0152274 ],\n",
              "        [0.01740689],\n",
              "        [0.0150995 ],\n",
              "        [0.0170441 ],\n",
              "        [0.02103939],\n",
              "        [0.01738417],\n",
              "        [0.01884246],\n",
              "        [0.02036481],\n",
              "        [0.02381245],\n",
              "        [0.0185755 ],\n",
              "        [0.01326161],\n",
              "        [0.01344259],\n",
              "        [0.01502453]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uOeDENHmeJj",
        "outputId": "ba3f251a-cca9-47c4-85f2-4cd358c93e8a"
      },
      "source": [
        "at.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"bahdanau_attention\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              multiple                  65792     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  257       \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   multiple                  0         \n",
            "=================================================================\n",
            "Total params: 131,841\n",
            "Trainable params: 131,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af3DaOXtRk7J",
        "outputId": "726df86e-c95b-4de3-9ae0-86f388afb3f2"
      },
      "source": [
        "tf.saved_model.save(at,path)\n",
        "converter=tf.lite.TFLiteConverter.from_saved_model(path)\n",
        "imported = tf.saved_model.load(path)\n",
        "print(imported.signatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/Image_captioning app/2/assets\n",
            "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, input_1, input_2) at 0x7F277F1BB4E0>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imVrPInGRk4Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQwHA2FjhYVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5482f9-7867-47bd-b4e3-82b9478777b8"
      },
      "source": [
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.em=tf.keras.layers.Embedding(5001,512)\n",
        "    self.dec=tf.keras.layers.GRU(256,activation='relu',return_state=True,return_sequences=True)\n",
        "    self.dense=tf.keras.layers.Dense(5001)\n",
        "    self.attention=BahdanauAttention(256)\n",
        "    #self.dense1=tf.keras.layers.Dense(512)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    enc_out,dec_hidden,inp=inputs\n",
        "    x=self.em(inp)\n",
        "    weights,context_vector=self.attention([dec_hidden,enc_out])\n",
        "    context_vector=tf.expand_dims(context_vector, 1)\n",
        "    concat=tf.keras.layers.concatenate([x,context_vector,dec_hidden],axis=2)\n",
        "    dec_out,dec_hidden=self.dec(concat)\n",
        "    #x=self.dense1(dec_out)\n",
        "    pred=self.dense(dec_out)\n",
        "    return dec_hidden,pred,weights\n",
        "  \n",
        "  \n",
        "dec=Decoder()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxBS7d8DiHqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773cbc3b-e317-462b-f2b7-4db760007396"
      },
      "source": [
        "# Testing Decoder\n",
        "\n",
        "inp=np.array([3]).reshape((1,1))\n",
        "input=[out,dec_hid,inp]\n",
        "dec_hidden,pred,weights=dec(input)\n",
        "\n",
        "assert weights.shape==(1,64,1), 'Somthing went wrong with weights'\n",
        "assert dec_hidden.shape==(1,256), 'Somthing went wrong with decoder hidden'\n",
        "assert pred.shape==(1,1,5001), 'Somthing went wrong with prediction vector'\n",
        "\n",
        "print('looks good')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiorbgBwmoFq",
        "outputId": "ea266497-066e-4d2f-d9c2-9982e4b7ebfe"
      },
      "source": [
        "dec.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  2560512   \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  multiple                  984576    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             multiple                  1285257   \n",
            "_________________________________________________________________\n",
            "bahdanau_attention_3 (Bahdan multiple                  131841    \n",
            "=================================================================\n",
            "Total params: 4,962,186\n",
            "Trainable params: 4,962,186\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPlUykOVR6cg",
        "outputId": "283c6e57-e34e-46e0-a476-7472ae3f7d3c"
      },
      "source": [
        "tf.saved_model.save(dec,path)\n",
        "converter=tf.lite.TFLiteConverter.from_saved_model(path)\n",
        "imported = tf.saved_model.load(path)\n",
        "print(imported.signatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/Image_captioning app/2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/Image_captioning app/2/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, input_1, input_2, input_3) at 0x7F278DAA80B8>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f79rDo0SAfK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is7I3sr3R6Z-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6WSmfl1R6W5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyA9UvbPkYoU"
      },
      "source": [
        "# Creating required APIs\n",
        "ls_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "acc=tf.keras.metrics.Accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y1PuEOhkYiT"
      },
      "source": [
        "# with autograph\n",
        "\n",
        "class Test_step():\n",
        "  def __init__(self,batch):\n",
        "    self.batch=batch\n",
        "    self.pred_out=[]\n",
        "    self.loss=0\n",
        "\n",
        "  @tf.function\n",
        "  def  __call__(self,inp,out):\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_out=enc(inp)\n",
        "      dec_hid=tf.zeros(shape=(self.batch,1,256))\n",
        "      for i in range(1,out.shape[1]):\n",
        "        dec_inp=tf.reshape(out[:,i-1],shape=(self.batch,1))\n",
        "        dec_hid,pred,weights=dec([enc_out,dec_hid,dec_inp])\n",
        "        dec_hid=tf.reshape(dec_hid,shape=(self.batch,1,dec_hid.shape[-1]))\n",
        "        self.pred_out.append(pred)\n",
        "\n",
        "      ypred=tf.concat(self.pred_out,axis=1)\n",
        "\n",
        "      ls=ls_(out[:,1:],ypred)\n",
        "\n",
        "    variables = enc.trainable_variables + dec.trainable_variables\n",
        "    gradients = tape.gradient(ls, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    #self.dec_hidden=dec_hid\n",
        "    return ls,ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emiPLrHPmVxH",
        "outputId": "5a896bfd-7aa0-4ac5-b159-a803e6e43291"
      },
      "source": [
        "# Lets train the model for 20 epochs\n",
        "# Training...\n",
        "batch=64\n",
        "#step=Test_step(batch)\n",
        "#trn=trn.shuffle(1000).batch(batch,drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "#trn=tf.data.Dataset.from_tensor_slices((trn,hn)).batch(batch,drop_remainder=True).shuffle(80000, reshuffle_each_iteration=True)\n",
        "epochs=6\n",
        "print('Training starts ...')\n",
        "for j in range(epochs):\n",
        "  t=time.time()\n",
        "  print('\\n\\n---Epoch {}  :----'.format(j+1))\n",
        "  for i,(inp,out) in enumerate(trn):\n",
        "    ls,ypred=step(inp,out)\n",
        "    ypred=tf.math.argmax(ypred,axis=2)\n",
        "    ypred=tf.reshape(ypred,shape=out[:,1:].shape)\n",
        "    ac=acc(ypred,out[:,1:])\n",
        "    if (i+1)%200==0:\n",
        "      print('At {}th batch    loss={} , accuracy={}'.format(i+1,ls,ac))\n",
        "\n",
        "  t=time.time()-t\n",
        "  print('\\nTime required for {}th epoch : {}'.format(j+1,t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training starts ...\n",
            "\n",
            "\n",
            "---Epoch 1  :----\n",
            "At 200th batch    loss=1.0193533897399902 , accuracy=0.7386730909347534\n",
            "At 400th batch    loss=0.8717350363731384 , accuracy=0.7388383746147156\n",
            "At 600th batch    loss=0.9103328585624695 , accuracy=0.7390006184577942\n",
            "At 800th batch    loss=0.9322250485420227 , accuracy=0.7391602396965027\n",
            "At 1000th batch    loss=0.9093019962310791 , accuracy=0.7393214106559753\n",
            "At 1200th batch    loss=1.12517249584198 , accuracy=0.7394710779190063\n",
            "At 1400th batch    loss=1.0463237762451172 , accuracy=0.7396174669265747\n",
            "At 1600th batch    loss=0.9422875046730042 , accuracy=0.7397670149803162\n",
            "At 1800th batch    loss=0.9286426305770874 , accuracy=0.7399148344993591\n",
            "At 2000th batch    loss=0.9277843236923218 , accuracy=0.7400665879249573\n",
            "At 2200th batch    loss=0.9376111626625061 , accuracy=0.7402229905128479\n",
            "\n",
            "Time required for 1th epoch : 439.3854115009308\n",
            "\n",
            "\n",
            "---Epoch 2  :----\n",
            "At 200th batch    loss=1.0289009809494019 , accuracy=0.740492045879364\n",
            "At 400th batch    loss=0.888620138168335 , accuracy=0.7406501173973083\n",
            "At 600th batch    loss=0.9866929650306702 , accuracy=0.7407980561256409\n",
            "At 800th batch    loss=0.9592037200927734 , accuracy=0.7409505844116211\n",
            "At 1000th batch    loss=0.8979483246803284 , accuracy=0.7411032319068909\n",
            "At 1200th batch    loss=0.9582670331001282 , accuracy=0.7412465810775757\n",
            "At 1400th batch    loss=0.9520502090454102 , accuracy=0.741385281085968\n",
            "At 1600th batch    loss=0.7973899841308594 , accuracy=0.7415270209312439\n",
            "At 1800th batch    loss=1.046546220779419 , accuracy=0.7416648268699646\n",
            "At 2000th batch    loss=0.8679512143135071 , accuracy=0.7418131828308105\n",
            "At 2200th batch    loss=0.8971928954124451 , accuracy=0.7419646978378296\n",
            "\n",
            "Time required for 2th epoch : 440.5012397766113\n",
            "\n",
            "\n",
            "---Epoch 3  :----\n",
            "At 200th batch    loss=1.0056177377700806 , accuracy=0.7422175407409668\n",
            "At 400th batch    loss=1.0188112258911133 , accuracy=0.7423669099807739\n",
            "At 600th batch    loss=0.9691594839096069 , accuracy=0.7425122857093811\n",
            "At 800th batch    loss=0.9576303362846375 , accuracy=0.7426539063453674\n",
            "At 1000th batch    loss=0.9145872592926025 , accuracy=0.7427982687950134\n",
            "At 1200th batch    loss=0.9830227494239807 , accuracy=0.7429364919662476\n",
            "At 1400th batch    loss=0.8907554149627686 , accuracy=0.7430659532546997\n",
            "At 1600th batch    loss=0.8759562969207764 , accuracy=0.7432036399841309\n",
            "At 1800th batch    loss=1.0013172626495361 , accuracy=0.7433379292488098\n",
            "At 2000th batch    loss=0.9029161334037781 , accuracy=0.7434781193733215\n",
            "At 2200th batch    loss=0.9163995981216431 , accuracy=0.7436279654502869\n",
            "\n",
            "Time required for 3th epoch : 439.96580719947815\n",
            "\n",
            "\n",
            "---Epoch 4  :----\n",
            "At 200th batch    loss=0.9133643507957458 , accuracy=0.7438663840293884\n",
            "At 400th batch    loss=0.9598057270050049 , accuracy=0.7440087199211121\n",
            "At 600th batch    loss=0.9074497222900391 , accuracy=0.7441422343254089\n",
            "At 800th batch    loss=0.8981420397758484 , accuracy=0.7442790269851685\n",
            "At 1000th batch    loss=1.0049775838851929 , accuracy=0.7444136738777161\n",
            "At 1200th batch    loss=0.9176450967788696 , accuracy=0.7445421814918518\n",
            "At 1400th batch    loss=0.9912856817245483 , accuracy=0.7446697354316711\n",
            "At 1600th batch    loss=0.8908019661903381 , accuracy=0.7447972297668457\n",
            "At 1800th batch    loss=0.934943437576294 , accuracy=0.7449248433113098\n",
            "At 2000th batch    loss=0.9510706067085266 , accuracy=0.7450557351112366\n",
            "At 2200th batch    loss=0.9513912200927734 , accuracy=0.7451938390731812\n",
            "\n",
            "Time required for 4th epoch : 439.5274143218994\n",
            "\n",
            "\n",
            "---Epoch 5  :----\n",
            "At 200th batch    loss=0.940329372882843 , accuracy=0.7454252243041992\n",
            "At 400th batch    loss=0.9166198968887329 , accuracy=0.7455576658248901\n",
            "At 600th batch    loss=0.9173476696014404 , accuracy=0.7456874251365662\n",
            "At 800th batch    loss=0.8679270148277283 , accuracy=0.7458130121231079\n",
            "At 1000th batch    loss=1.040609359741211 , accuracy=0.7459424734115601\n",
            "At 1200th batch    loss=0.9374393224716187 , accuracy=0.7460639476776123\n",
            "At 1400th batch    loss=0.9719228148460388 , accuracy=0.7461833953857422\n",
            "At 1600th batch    loss=0.9062133431434631 , accuracy=0.7463005185127258\n",
            "At 1800th batch    loss=0.8597230315208435 , accuracy=0.746423065662384\n",
            "At 2000th batch    loss=0.9015527963638306 , accuracy=0.7465487718582153\n",
            "At 2200th batch    loss=0.9209926128387451 , accuracy=0.7466761469841003\n",
            "\n",
            "Time required for 5th epoch : 439.98410153388977\n",
            "\n",
            "\n",
            "---Epoch 6  :----\n",
            "At 200th batch    loss=0.8967195153236389 , accuracy=0.7468956112861633\n",
            "At 400th batch    loss=0.8550965785980225 , accuracy=0.7470219135284424\n",
            "At 600th batch    loss=0.893702507019043 , accuracy=0.7471411228179932\n",
            "At 800th batch    loss=0.8629869222640991 , accuracy=0.7472617030143738\n",
            "At 1000th batch    loss=0.8388168811798096 , accuracy=0.7473852038383484\n",
            "At 1200th batch    loss=1.0399699211120605 , accuracy=0.7475008368492126\n",
            "At 1400th batch    loss=0.9416504502296448 , accuracy=0.7476150989532471\n",
            "At 1600th batch    loss=0.9027017951011658 , accuracy=0.7477307319641113\n",
            "At 1800th batch    loss=0.8960907459259033 , accuracy=0.7478458881378174\n",
            "At 2000th batch    loss=0.946964681148529 , accuracy=0.7479638457298279\n",
            "At 2200th batch    loss=0.9402329921722412 , accuracy=0.7480869293212891\n",
            "\n",
            "Time required for 6th epoch : 439.9447853565216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7yhomX6mVrI"
      },
      "source": [
        " ;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NFTyetP48ev"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pvavYTbX-Ku"
      },
      "source": [
        "#Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sezB_A0YmfA"
      },
      "source": [
        "#reference\n",
        "#class Model(tf.keras.Model):\n",
        "  def __init__(self,enc,dec):\n",
        "    super(Model,self).__init__()\n",
        "    self.enc=enc\n",
        "    self.dec=dec\n",
        "\n",
        "  @tf.function\n",
        "  def  call (self,inp):\n",
        "    result=[]\n",
        "    enc_out,dec_hid=self.enc(inp)\n",
        "    dec_inp=tf.reshape(inp[:,0],shape=[1,1])\n",
        "    for i in range(1,13):\n",
        "      dec_hid=tf.reshape(dec_hid,shape=(1,1,dec_hid.shape[-1]))\n",
        "      dec_hid,pred,weights=self.dec([enc_out,dec_hid,dec_inp])\n",
        "      dec_inp=tf.reshape(tf.math.argmax(pred,axis=2),shape=(1,1))\n",
        "      result.append(dec_inp)\n",
        "    o=tf.concat(result,axis=1)\n",
        "    o=tf.squeeze(o,axis=[0])\n",
        "    return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM8vh_piYAMl"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self,enc,dec,inception_feature_extraction):\n",
        "    super(Model,self).__init__()\n",
        "    self.enc=enc\n",
        "    self.dec=dec\n",
        "    self.inception_feature_extraction=inception_feature_extraction\n",
        "    #self.con=tf.constant([3],tf.int32,(1,1))\n",
        "\n",
        "  @tf.function\n",
        "  def call (self,inp):\n",
        "    result=[]\n",
        "    i=self.inception_feature_extraction(inp)\n",
        "    i=tf.reshape(i,(1,64,2048))\n",
        "    enc_out=self.enc(i)\n",
        "    dec_hid=tf.zeros(shape=(1,1,256))\n",
        "    #dec_inp=tf.constant(inp[:,0],shape=[1,1])\n",
        "    dec_inp=tf.constant([3],tf.int32,(1,1))\n",
        "    for i in range(1,23):\n",
        "      dec_hid=tf.reshape(dec_hid,shape=(1,1,dec_hid.shape[-1]))\n",
        "      dec_hid,pred,weights=self.dec([enc_out,dec_hid,dec_inp])\n",
        "      dec_inp=tf.reshape(tf.math.argmax(pred,axis=2),shape=(1,1))\n",
        "      result.append(dec_inp)\n",
        "    o=tf.concat(result,axis=1)\n",
        "    o=tf.squeeze(o,axis=[0])\n",
        "    return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Tr4iuLcduI"
      },
      "source": [
        "model=Model(enc,dec,inception_feature_extraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXgyQSUrcwwe"
      },
      "source": [
        "inp=image_paths[10]\n",
        "i=tf.io.read_file(inp)\n",
        "i=tf.image.decode_jpeg(i, channels=3)\n",
        "i=tf.image.resize(i, (299, 299))\n",
        "i=tf.keras.applications.inception_v3.preprocess_input(i)\n",
        "i=tf.expand_dims(i,0)\n",
        "i=i.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ElhIW_Bc5z_",
        "outputId": "dc69c200-c2e2-47fd-afe0-55c96fa6d9ac"
      },
      "source": [
        "model(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(22,), dtype=int64, numpy=\n",
              "array([  2, 285,   6, 110,  10,  22, 242,   8,   2, 628, 100,   4,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0UyntTldYf9"
      },
      "source": [
        "path='/content/drive/MyDrive/models/Image_captioning app/1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRcdLHmce560",
        "outputId": "cdcb4946-2eac-400d-e139-eeba8fb24bb4"
      },
      "source": [
        "tf.saved_model.save(\n",
        "    model, path,\n",
        "    signatures=model.call.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[1, 299, 299, 3], dtype=tf.float32, name=\"inp\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/Image_captioning app/1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wGAP0ToDG6TU",
        "outputId": "f716b6e5-7cf7-4419-d470-f13673cf628f"
      },
      "source": [
        "pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/d1/b120060acaf0377bbabc219c24ee9e4692b4f96d90fa7154bc923c45af27/tf_nightly-2.5.0.dev20210122-cp36-cp36m-manylinux2010_x86_64.whl (401.0MB)\n",
            "\u001b[K     || 401.0MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Collecting grpcio~=1.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/f5/3d3bcb82beae990021cbf6877456a1aab650e68c902194566edd6a73e37c/grpcio-1.34.1-cp36-cp36m-manylinux2014_x86_64.whl (4.0MB)\n",
            "\u001b[K     || 4.0MB 57.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Collecting tf-estimator-nightly~=2.5.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/13/bc5258d667fc2f64ea1a57ad416f204fdf8bed040bf0140f865f9c1936a7/tf_estimator_nightly-2.5.0.dev2021012201-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     || 471kB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Collecting tb-nightly~=2.5.0.a\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/5a/5ac79139cb2f5d41a9ea4358a81754da9c05af7e01c1445f99e30c2b5fab/tb_nightly-2.5.0a20210122-py3-none-any.whl (12.2MB)\n",
            "\u001b[K     || 12.2MB 255kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.19.5)\n",
            "Collecting h5py~=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     || 4.0MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (51.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: grpcio, gast, tf-estimator-nightly, tb-nightly, cached-property, h5py, tf-nightly\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "Successfully installed cached-property-1.5.2 gast-0.4.0 grpcio-1.34.1 h5py-3.1.0 tb-nightly-2.5.0a20210122 tf-estimator-nightly-2.5.0.dev2021012201 tf-nightly-2.5.0.dev20210122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "grpc",
                  "h5py",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oucsFsamfIcP",
        "outputId": "f06fc061-4767-4d45-fba9-1377522febd1"
      },
      "source": [
        "converter=tf.lite.TFLiteConverter.from_saved_model(path)\n",
        "imported = tf.saved_model.load(path)\n",
        "print(imported.signatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, inp) at 0x7F169C6224A8>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-1jFIkG7Js0"
      },
      "source": [
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ktAP3-8VH3oH",
        "outputId": "868359cd-58af-4d43-fa16-993edb323c26"
      },
      "source": [
        "pa='/content/drive/MyDrive/models/Image_captioning app/model.tflite'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/models/Image_captioning app/1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek9aEUUoIJQ-"
      },
      "source": [
        "with open('/content/drive/MyDrive/models/Image_captioning app/model.tflite','wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPd-zQrfI9FW"
      },
      "source": [
        "with open('/content/wd_itow') as f:\n",
        "  d=json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnVoMH5_7ayW"
      },
      "source": [
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTyW7WLo7qPE"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvzASbxT7vwu"
      },
      "source": [
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGHJ2OEy72jX"
      },
      "source": [
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ymg-cA7-JD"
      },
      "source": [
        "interpreter.invoke()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdiUu5Dj8MCl",
        "outputId": "5e0fd247-abe3-4efd-fe60-723dd1316bff"
      },
      "source": [
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iSqW7rj8RKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8db63e4-3a9c-4b5c-b144-b6aea2b2f612"
      },
      "source": [
        "type(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZdAGljqtJOPt",
        "outputId": "5b852746-3353-46eb-d95a-474589342e4f"
      },
      "source": [
        "d['2']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82IKX3C1JP2M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}